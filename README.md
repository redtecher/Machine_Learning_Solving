# 一、感知机算法的更新规则 percepto.md
感知机算法的更新规则为：
\[ \text{IF } \quad \text{sign}(w_t^\top x_i) \neq y_i, \quad \text{THEN } \quad w_{t+1} \leftarrow w_t + y_i x_i. \]

#### (1) \( w_{t+1} \) 是否一定能将 \( x_i \) 分对？如果不可以，请设计新的更新准则以确保 \( x_i \) 能被 \( w_{t+1} \) 分对。
- **回答**：感知机算法的更新规则保证了如果当前权重 \( w_t \) 对样本 \( x_i \) 分类错误，则更新后的权重 \( w_{t+1} \) 会使得 \( \text{sign}(w_{t+1}^\top x_i) = y_i \)，即 \( w_{t+1} \) 能够正确分类 \( x_i \)。因此，\( w_{t+1} \) 一定能将 \( x_i \) 分对。

#### (2) 假设样本集 \(\{(x_i, y_i)\}_{i=1}^n\) 线性可分且被中心化，\( w_f \) 为存在但未知的理想线性分类器的法向量。定义 \( R^2 = \max_i \|x_i\|^2 \) 及 \( \rho = \min_i \frac{y_i w_f^\top x_i}{\|w_f\|} \)。试推导感知机算法达到收敛条件所需要的迭代次数的上界。
- **回答**：感知机算法在数据线性可分的情况下，收敛次数的上界可以通过以下公式计算：
  \[ T \leq \frac{R^2}{\rho^2} \]
  其中 \( R^2 = \max_i \|x_i\|^2 \) 是样本点的最大范数平方，\( \rho = \min_i \frac{y_i w_f^\top x_i}{\|w_f\|} \) 是样本点到超平面的最小距离。

#### (3) 如何改进线性感知机算法使得它能够处理非线性可分的数据。
- **回答**：对于非线性可分的数据，可以采用以下几种方法改进感知机算法：
  - **核技巧（Kernel Trick）**：通过引入核函数将低维空间中的非线性可分数据映射到高维空间，使其变得线性可分。
  - **多层感知机（Multilayer Perceptron, MLP）**：通过增加网络层数和引入非线性激活函数，使模型具有更强的表达能力，能够处理非线性关系。
  - **支持向量机（Support Vector Machine, SVM）**：使用软间隔和核函数来处理非线性可分数据。

# 二、证明：对于给定条件下的随机梯度下降（SGD）算法，其收敛速率满足以下不等式：sgd.md

\[
\mathbb{E}[f(\hat{w})] - f(w^*) \leq \frac{B \rho}{\sqrt{T}}
\]

其中：
- \( f(\cdot) \) 是一个凸函数，且 \( w^* \in \arg\min_{w: \|w\| \leq B} f(w) \)。
- \( \hat{w} = \frac{1}{T} \sum_{t=1}^{T} w_t \) 是 SGD 算法在 T 次迭代后得到的平均解。
- 学习速率 \( \eta = \sqrt{\frac{B^2}{T \rho^2}} \)。
- 随机梯度的噪声（即每次迭代的下降方向）满足 \( \| v_t \| \leq \rho \)（这是一个有界假设）。

# 三、支持向量机（SVM）svm.md
#### (1) 支持向量机（SVM）的原始优化目标及对偶形式推导
#### (2) 设计随机梯度下降算法以高效地求解大规模支持向量机对偶问题.
#### (3) 假设空间中的经验期望问题,证明
\[
     \mathbb{E}[h] \leq \hat{\mathbb{E}}[h] + \mathcal{R}_n(\mathcal{H}) + \sqrt{\frac{\ln(1/\delta)}{2n}}
     \]

# 四、自动驾驶汽车和反向传播 automaticar.md

你想要训练一个神经网络来驾驶一辆汽车。你的训练数据包含 \(64 \times 64\) 像素的灰度图。训练标签包含人类驾驶员方向盘角度（度）和人类驾驶员的速度（英里/小时）。你的神经网络包含一个 \(64 \times 64 = 4,096\) 单元的输入层，一个 2,048 单元的隐藏层，以及一个 2 个单元的输出层（一个是转向角，一个是速度）。对隐藏单元使用 ReLU 激活函数，输入或输出没有激活函数。

#### (1) (5分)
计算这个网络中的参数（权重）的数量。

#### (2) (10分)
使用损失函数 \(J = \frac{1}{2} \| z - y \|^2\) 训练网络。使用以下符号：
- \(x\) 是一个输入的训练图像向量，在最后附加了一个分量 “1”，\(y\) 是输入的训练标签向量，\(z\) 是输出向量。
- \(r(\gamma) = \max\{0, \gamma\}\) 是 ReLU 激活函数，\(r'(\gamma)\) 是其导数 (\(\gamma > 0\) 时为 1，否则为 0)，\(r(v)\) 是 \(r(\cdot)\) 按分量应用到一个向量上。
- \(g\) 是应用 ReLU 激活函数之前的隐藏单元值的向量，\(h = r(g)\) 是应用后的隐藏单元值的向量 (但是在 \(h\) 的末尾添加了一个分量 “1”)。
- \(V\) 是将输入层映射到隐藏层的权重矩阵，\(g = Vx\)。
- \(W\) 是将隐藏层映射到输出层的权重矩阵，\(z = Wh\)。

求 \(\frac{\partial J}{\partial W_{ij}}\)。

#### (3) (5分)
求 \(\frac{\partial J}{\partial W}\)。

#### (4) (5分)
求 \(\frac{\partial J}{\partial V_{ij}}\)。

# 五、K-means k-means.md

回顾 k-means 聚类的损失函数，其有着 \( k \) 个簇，样本点 \( x_1, \cdots, x_n \)，以及中心点 \( \mu_1, \cdots, \mu_k \)：
\[
L = \frac{1}{2} \sum_{j=1}^k \sum_{x_i \in S_j} \| x_i - \mu_j \|^2
\]
其中，\( S_j \) 指的是相比较于其他簇的均值，更接近 \( \mu_j \) 的样本点的集合。

---

**问题 1 (5 分)：** 不采取通过计算均值的方法来更新 \( \mu_j \)，我们在保持集合 \( S_j \) 固定时用批量梯度下降法来最小化损失函数 \( L \)。试推导出学习率（步长）为 \( \epsilon \) 时 \( \mu_1 \) 的更新公式。

---

**问题 2 (5 分)：** 推导出在单一样本点 \( x_i \) 使用随机梯度下降时 \( \mu_1 \) 的更新公式，学习率设置为 \( \epsilon \)。

---

**问题 3 (5 分)：** 在这一部分，我们将把批量梯度下降更新公式与标准的 k-means 算法联系起来。回顾一下，在标准 k-means 算法的更新步骤中，我们将每个簇中心分配为最接近该中心的数据显示点的平均值（中心点）。事实证明，对学习率 \( \epsilon \) 的特定选择（对每个簇可能不同）使得两种算法（批量梯度下降和标准 k-means 算法）具有相同的更新步骤。让我们关注第一个簇的更新，其中心点为 \( \mu_1 \)。计算 \( \epsilon \) 的值，使得两种算法对 \( \mu_1 \) 进行相同的更新。

---

# 六、K 均值 (K-means) 聚类算法和谱聚类算法的区别 K-meas-description.md
如何处理特征缺失的数据聚类


# 七、复杂预测问题 duplicateproblem.md
给定样本集合 \( \mathcal{S} = \{(x_1, x'_1, y_1), \cdots, (x_n, x'_n, y_n)\} \in \mathcal{X} \times \mathcal{X} \times \{+1, -1\} \)，其中 \( (x_1, x'_1), \cdots, (x_n, x'_n) \) 独立同分布，\( y_i = f(x_i, x'_i), \forall i \)。

1. (5') 给出排序问题的假设集 \( \mathcal{H} \)。
2. (5') 基于该假设集，设计排序算法的优化目标。
3. (5') 设计对应的求解算法。

