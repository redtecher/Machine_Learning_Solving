好的，以下是图片中的内容：

---

### 五、自动驾驶汽车和反向传播（25分）

你想要训练一个神经网络来驾驶一辆汽车。你的训练数据包含 \(64 \times 64\) 像素的灰度图。训练标签包含人类驾驶员方向盘角度（度）和人类驾驶员的速度（英里/小时）。你的神经网络包含一个 \(64 \times 64 = 4,096\) 单元的输入层，一个 2,048 单元的隐藏层，以及一个 2 个单元的输出层（一个是转向角，一个是速度）。对隐藏单元使用 ReLU 激活函数，输入或输出没有激活函数。

#### (1) (5分)
计算这个网络中的参数（权重）的数量。

#### (2) (10分)
使用损失函数 \(J = \frac{1}{2} \| z - y \|^2\) 训练网络。使用以下符号：
- \(x\) 是一个输入的训练图像向量，在最后附加了一个分量 “1”，\(y\) 是输入的训练标签向量，\(z\) 是输出向量。
- \(r(\gamma) = \max\{0, \gamma\}\) 是 ReLU 激活函数，\(r'(\gamma)\) 是其导数 (\(\gamma > 0\) 时为 1，否则为 0)，\(r(v)\) 是 \(r(\cdot)\) 按分量应用到一个向量上。
- \(g\) 是应用 ReLU 激活函数之前的隐藏单元值的向量，\(h = r(g)\) 是应用后的隐藏单元值的向量 (但是在 \(h\) 的末尾添加了一个分量 “1”)。
- \(V\) 是将输入层映射到隐藏层的权重矩阵，\(g = Vx\)。
- \(W\) 是将隐藏层映射到输出层的权重矩阵，\(z = Wh\)。

求 \(\frac{\partial J}{\partial W_{ij}}\)。

#### (3) (5分)
求 \(\frac{\partial J}{\partial W}\)。

#### (4) (5分)
求 \(\frac{\partial J}{\partial V_{ij}}\)。

### 问题解答

#### (1) 计算神经网络中的参数数量

神经网络的参数数量指的是权重和偏置的总数。我们来逐一计算每一层的参数数量。

- **输入层到隐藏层的权重**：输入层有 \(4,096\) 个单元（包括图像的每个像素），隐藏层有 \(2,048\) 个单元。
  - 每个隐藏单元有 \(4,096\) 个输入，因此权重矩阵的大小是 \(2,048 \times 4,096\)。
  - 所以输入层到隐藏层的权重数量是：  
    \[
    2,048 \times 4,096 = 8,388,608
    \]

- **隐藏层到输出层的权重**：隐藏层有 \(2,048\) 个单元，输出层有 2 个单元。
  - 每个输出单元有 \(2,048\) 个输入，因此权重矩阵的大小是 \(2 \times 2,048\)。
  - 所以隐藏层到输出层的权重数量是：  
    \[
    2 \times 2,048 = 4,096
    \]

- **偏置**：
  - 对于隐藏层有 \(2,048\) 个偏置。
  - 对于输出层有 2 个偏置。

- **总参数数量**：
  - 输入层到隐藏层的权重：\(8,388,608\)
  - 隐藏层到输出层的权重：\(4,096\)
  - 隐藏层的偏置：\(2,048\)
  - 输出层的偏置：\(2\)

  所以，神经网络的总参数数量是：
  \[
  8,388,608 + 4,096 + 2,048 + 2 = 8,394,754
  \]

#### (2) 计算 \(\frac{\partial J}{\partial W_{ij}}\)

损失函数为：
\[
J = \frac{1}{2} \| z - y \|^2
\]
其中 \(z\) 是输出，\(y\) 是标签。我们要求 \(\frac{\partial J}{\partial W_{ij}}\)，其中 \(W\) 是从隐藏层到输出层的权重矩阵。

根据链式法则，首先我们计算损失函数 \(J\) 关于输出 \(z\) 的梯度：
\[
\frac{\partial J}{\partial z_i} = z_i - y_i
\]
接下来，我们需要计算 \(z\) 关于权重矩阵 \(W\) 的梯度：
\[
z = Wh
\]
其中 \(h = r(g)\)，并且 \(g = Vx\)，其中 \(V\) 是从输入层到隐藏层的权重矩阵。

对于每个输出单元 \(z_i\)，我们有：
\[
\frac{\partial J}{\partial W_{ij}} = \frac{\partial J}{\partial z_i} \frac{\partial z_i}{\partial W_{ij}} = (z_i - y_i) h_j
\]
其中 \(h_j\) 是隐藏层的第 \(j\) 个单元。

所以，\(\frac{\partial J}{\partial W_{ij}} = (z_i - y_i) h_j\)。

#### (3) 计算 \(\frac{\partial J}{\partial W}\)

从 (2) 中得知：
\[
\frac{\partial J}{\partial W_{ij}} = (z_i - y_i) h_j
\]
因此，整个 \(W\) 的梯度为：
\[
\frac{\partial J}{\partial W} = (z - y) h^T
\]
其中 \(h^T\) 是隐藏层值 \(h\) 的转置。

#### (4) 计算 \(\frac{\partial J}{\partial V_{ij}}\)

接下来，我们计算损失函数关于 \(V\) 的梯度。首先，回顾 \(g = Vx\)，而且 \(h = r(g)\)。梯度计算过程如下：

我们需要先求 \(J\) 关于 \(g\) 的梯度。通过链式法则，我们有：
\[
\frac{\partial J}{\partial g_j} = \sum_i \frac{\partial J}{\partial z_i} \frac{\partial z_i}{\partial g_j}
\]
其中 \(\frac{\partial z_i}{\partial g_j} = W_{ij}\)，所以：
\[
\frac{\partial J}{\partial g_j} = \sum_i (z_i - y_i) W_{ij}
\]

然后，由于 \(h_j = r(g_j)\)，我们有：
\[
\frac{\partial J}{\partial g_j} = \frac{\partial J}{\partial h_j} \cdot r'(g_j)
\]
其中 \(r'(g_j)\) 为 ReLU 激活函数的导数，等于 1 当 \(g_j > 0\) 时，否则为 0。

最后，计算 \(\frac{\partial J}{\partial V_{ij}}\)：
\[
\frac{\partial J}{\partial V_{ij}} = \frac{\partial J}{\partial g_j} x_i
\]
其中 \(x_i\) 是输入数据中的第 \(i\) 个分量。

因此，\(\frac{\partial J}{\partial V_{ij}} = \left( \sum_i (z_i - y_i) W_{ij} \right) r'(g_j) x_i\)。

总结：
- (1) 神经网络中的参数总数是 8,394,754。
- (2) \(\frac{\partial J}{\partial W_{ij}} = (z_i - y_i) h_j\)。
- (3) \(\frac{\partial J}{\partial W} = (z - y) h^T\)。
- (4) \(\frac{\partial J}{\partial V_{ij}} = \left( \sum_i (z_i - y_i) W_{ij} \right) r'(g_j) x_i\)。